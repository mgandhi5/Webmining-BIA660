{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>HW #1</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">Each assignment needs to be completed independently. Never ever copy others' work (even with minor modification, e.g. changing variable names). Anti-Plagiarism software will be used to check all submissions. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions**: \n",
    "- Please read the problem description carefully\n",
    "- Make sure to complete all requirements (shown as bullets) . In general, it would be much easier if you complete the requirements in the order as shown in the problem description\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem Description**\n",
    "\n",
    "In this assignment, you'll write functions to analyze an article to find out the word distributions and key concepts. \n",
    "\n",
    "The packages you'll need for this assignment include numpy and pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. Define a function to analyze word counts in an input sentence \n",
    "\n",
    "\n",
    "Define a function named `tokenize(text)` which does the following:\n",
    "* accepts a sentence (i.e., `text` parameter) as an input\n",
    "* splits the sentence into a list of tokens by **space** (including tab, and new line). \n",
    "    - e.g., `it's a hello world!!!` will be split into tokens `[\"it's\", \"a\",\"hello\",\"world!!!\"]`  \n",
    "* removes the **leading/trailing punctuations or spaces** of each token, if any\n",
    "    - e.g., `world!!! -> world`, while `it's` does not change\n",
    "    - hint, you can import module *string*, use `string.punctuation` to get a list of punctuations (say `puncts`), and then use function `strip(puncts)` to remove leading or trailing punctuations in each token\n",
    "* only keeps tokens with 2 or more characters, i.e. `len(token)>1` \n",
    "* converts all tokens into lower case \n",
    "* find the count of each unique token and save the counts as dictionary, i.e., `{world: 1, a: 1, ...}`\n",
    "* returns the dictionary \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def tokenize(text):\n",
    "    \n",
    "    \n",
    "    tokens=[]\n",
    "    d=dict()\n",
    "    text=pd.Series(text)\n",
    "    \n",
    "    #tokens = text.split()\n",
    "    #tokens = [''.join(a for a in s if a not in string.punctuation) for s in tokens]\n",
    "    #tokens = [token for token in tokens if len(token) > 1]\n",
    "    #tokens=dict(zip(list(tokens),[list(tokens).count(i) for i in list(tokens)]))  \n",
    "   \n",
    "    for token in text:\n",
    "        token = token.translate(str.maketrans('', '', string.punctuation))\n",
    "        token=token.split(\" \")\n",
    "        for word in token:\n",
    "            word=word.lower()\n",
    "            if len(word)>1:\n",
    "            \n",
    "                if word not in d:\n",
    "                    d[word] = 1\n",
    "                else:\n",
    "                    d[word] = d[word]+1\n",
    "            \n",
    "    \n",
    "        \n",
    "            \n",
    "        \n",
    "    #tokens = list(filter(None, tokens))\n",
    "    #tokens = [x.lower() for x in tokens]\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'its': 1, 'hello': 2, 'world\\n': 1, 'it': 1, 'is': 1, 'world': 1, 'again': 1}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test your code\n",
    "text = \"\"\"it's a hello world!!!\n",
    "           it is hello world again.\"\"\"\n",
    "tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. Generate a document term matrix (DTM) as a numpy array\n",
    "\n",
    "\n",
    "Define a function `get_dtm(sents)` as follows:\n",
    "- accepts a list of sentences, i.e., `sents`, as an input\n",
    "- uses `tokenize` function you defined in Q1 to get the count dictionary for each sentence\n",
    "- pools the words from all the strings togehter to get a list of  unique words, denoted as `unique_words`\n",
    "- creates a numpy array, say `dtm` with a shape (# of docs x # of unique words), and set the initial values to 0.\n",
    "- fills cell `dtm[i,j]` with the count of the `j`th word in the `i`th sentence\n",
    "- returns `dtm` and `unique_words`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_dtm(sents):\n",
    "    \n",
    "    \n",
    "    \n",
    "    dtm= len(sents)\n",
    "    words = \" \"\n",
    "\n",
    "    words=tokenize(sents)\n",
    "\n",
    "    dtm=np.zeros((len(sents),len(words)))\n",
    "    \n",
    "    for i in range(len(sents)):\n",
    "        unique_words=tokenize(sents[i])\n",
    "        for tokens,d in unique_words.items():\n",
    "\n",
    "            dtm[i][list(words.keys()).index(tokens)]=d\n",
    "\n",
    "    return dtm, words\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1., 1., 1., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [2., 0., 2., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 0., 1., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 1., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 1., 1.]]),\n",
       " {'the': 68,\n",
       "  'power': 1,\n",
       "  'of': 50,\n",
       "  'natural': 3,\n",
       "  'language': 12,\n",
       "  'processing': 4,\n",
       "  'until': 1,\n",
       "  'recently': 3,\n",
       "  'conventional': 1,\n",
       "  'wisdom': 1,\n",
       "  'was': 7,\n",
       "  'that': 17,\n",
       "  'while': 4,\n",
       "  'ai': 25,\n",
       "  'better': 7,\n",
       "  'than': 5,\n",
       "  'humans': 5,\n",
       "  'at': 7,\n",
       "  'datadriven': 1,\n",
       "  'decision': 2,\n",
       "  'making': 1,\n",
       "  'tasks': 20,\n",
       "  'it': 15,\n",
       "  'still': 7,\n",
       "  'inferior': 1,\n",
       "  'to': 65,\n",
       "  'for': 37,\n",
       "  'cognitive': 4,\n",
       "  'and': 52,\n",
       "  'creative': 1,\n",
       "  'ones': 1,\n",
       "  'but': 18,\n",
       "  'in': 24,\n",
       "  'past': 1,\n",
       "  'two': 1,\n",
       "  'years': 3,\n",
       "  'languagebased': 8,\n",
       "  'has': 8,\n",
       "  'advanced': 5,\n",
       "  'by': 6,\n",
       "  'leaps': 1,\n",
       "  'bounds': 1,\n",
       "  'changing': 2,\n",
       "  'common': 1,\n",
       "  'notions': 1,\n",
       "  'what': 4,\n",
       "  'this': 14,\n",
       "  'technology': 1,\n",
       "  'can': 17,\n",
       "  'dothe': 1,\n",
       "  'most': 5,\n",
       "  'visible': 1,\n",
       "  'advances': 1,\n",
       "  'have': 6,\n",
       "  'been': 5,\n",
       "  'what’s': 1,\n",
       "  'called': 3,\n",
       "  '“natural': 1,\n",
       "  'processing”': 1,\n",
       "  'nlp': 5,\n",
       "  'branch': 1,\n",
       "  'focused': 1,\n",
       "  'on': 13,\n",
       "  'how': 9,\n",
       "  'computers': 1,\n",
       "  'process': 2,\n",
       "  'like': 20,\n",
       "  'do': 7,\n",
       "  'used': 5,\n",
       "  'write': 1,\n",
       "  'an': 10,\n",
       "  'article': 2,\n",
       "  'guardian': 1,\n",
       "  'aiauthored': 1,\n",
       "  'blog': 1,\n",
       "  'posts': 1,\n",
       "  'gone': 1,\n",
       "  'viral': 1,\n",
       "  'feats': 1,\n",
       "  'weren’t': 1,\n",
       "  'possible': 2,\n",
       "  'few': 1,\n",
       "  'ago': 1,\n",
       "  'even': 8,\n",
       "  'excels': 1,\n",
       "  'programming': 4,\n",
       "  'where': 2,\n",
       "  'is': 23,\n",
       "  'able': 1,\n",
       "  'generate': 3,\n",
       "  'programs': 1,\n",
       "  'simple': 2,\n",
       "  'video': 3,\n",
       "  'games': 1,\n",
       "  'from': 10,\n",
       "  'human': 2,\n",
       "  'instructions': 2,\n",
       "  'yet': 2,\n",
       "  'these': 6,\n",
       "  'stunts': 1,\n",
       "  'may': 8,\n",
       "  'be': 20,\n",
       "  'attention': 2,\n",
       "  'grabbing': 1,\n",
       "  'are': 22,\n",
       "  'they': 6,\n",
       "  'really': 1,\n",
       "  'indicative': 1,\n",
       "  'tech': 4,\n",
       "  'businesses': 2,\n",
       "  'best': 3,\n",
       "  'known': 1,\n",
       "  'tool': 4,\n",
       "  'gpt3': 7,\n",
       "  'openai': 4,\n",
       "  'which': 7,\n",
       "  'uses': 1,\n",
       "  'statistics': 1,\n",
       "  'predict': 2,\n",
       "  'next': 4,\n",
       "  'word': 2,\n",
       "  'sentence': 1,\n",
       "  'based': 1,\n",
       "  'preceding': 1,\n",
       "  'words': 1,\n",
       "  'practitioners': 1,\n",
       "  'call': 1,\n",
       "  'tools': 13,\n",
       "  '“language': 1,\n",
       "  'models”': 1,\n",
       "  'analytics': 4,\n",
       "  'such': 5,\n",
       "  'as': 7,\n",
       "  'classifying': 1,\n",
       "  'documents': 1,\n",
       "  'analyzing': 1,\n",
       "  'sentiment': 1,\n",
       "  'blocks': 1,\n",
       "  'text': 10,\n",
       "  'well': 3,\n",
       "  'more': 8,\n",
       "  'answering': 1,\n",
       "  'questions': 3,\n",
       "  'summarizing': 1,\n",
       "  'reports': 2,\n",
       "  'models': 11,\n",
       "  'already': 5,\n",
       "  'reshaping': 1,\n",
       "  'traditional': 3,\n",
       "  'especially': 1,\n",
       "  'pivotal': 1,\n",
       "  'model': 8,\n",
       "  'because': 5,\n",
       "  '10x': 1,\n",
       "  'larger': 1,\n",
       "  'any': 2,\n",
       "  'previous': 2,\n",
       "  'upon': 1,\n",
       "  'release': 1,\n",
       "  'first': 2,\n",
       "  'large': 2,\n",
       "  'enabled': 1,\n",
       "  'perform': 1,\n",
       "  'solving': 1,\n",
       "  'high': 1,\n",
       "  'school–level': 1,\n",
       "  'math': 1,\n",
       "  'problems': 1,\n",
       "  'latest': 5,\n",
       "  'version': 1,\n",
       "  'instructgpt': 1,\n",
       "  'finetuned': 1,\n",
       "  'responses': 1,\n",
       "  'much': 3,\n",
       "  'aligned': 1,\n",
       "  'with': 6,\n",
       "  'values': 1,\n",
       "  'user': 1,\n",
       "  'intentions': 1,\n",
       "  'google’s': 3,\n",
       "  'shows': 2,\n",
       "  'further': 1,\n",
       "  'impressive': 1,\n",
       "  'breakthroughs': 1,\n",
       "  'reasoning': 3,\n",
       "  'three': 2,\n",
       "  'areas': 1,\n",
       "  'appeared': 1,\n",
       "  'promising': 1,\n",
       "  'writing': 2,\n",
       "  'coding': 1,\n",
       "  'disciplinespecific': 1,\n",
       "  'microsoftfunded': 1,\n",
       "  'creator': 1,\n",
       "  'developed': 1,\n",
       "  'gpt3based': 2,\n",
       "  'intended': 2,\n",
       "  'act': 1,\n",
       "  'assistant': 3,\n",
       "  'programmers': 5,\n",
       "  'generating': 2,\n",
       "  'code': 1,\n",
       "  'input': 1,\n",
       "  'codex': 2,\n",
       "  'powering': 1,\n",
       "  'products': 1,\n",
       "  'copilot': 1,\n",
       "  'microsoft’s': 1,\n",
       "  'subsidiary': 1,\n",
       "  'github': 1,\n",
       "  'capable': 1,\n",
       "  'creating': 2,\n",
       "  'basic': 1,\n",
       "  'game': 1,\n",
       "  'simply': 3,\n",
       "  'typing': 1,\n",
       "  'transformative': 6,\n",
       "  'capability': 1,\n",
       "  'expected': 1,\n",
       "  'change': 1,\n",
       "  'nature': 3,\n",
       "  'their': 6,\n",
       "  'jobs': 2,\n",
       "  'continue': 2,\n",
       "  'improve': 1,\n",
       "  'deepmind': 2,\n",
       "  'lab': 1,\n",
       "  'example': 2,\n",
       "  'demonstrates': 1,\n",
       "  'critical': 3,\n",
       "  'thinking': 2,\n",
       "  'logic': 1,\n",
       "  'skills': 2,\n",
       "  'necessary': 1,\n",
       "  'outperform': 1,\n",
       "  'competitions': 1,\n",
       "  'considered': 1,\n",
       "  'foundation': 6,\n",
       "  'emerging': 2,\n",
       "  'research': 10,\n",
       "  'area': 1,\n",
       "  'also': 3,\n",
       "  'work': 7,\n",
       "  'other': 6,\n",
       "  'types': 1,\n",
       "  'data': 19,\n",
       "  'images': 2,\n",
       "  'trained': 2,\n",
       "  'multiple': 1,\n",
       "  'forms': 1,\n",
       "  'same': 1,\n",
       "  'time': 2,\n",
       "  'openai’s': 2,\n",
       "  'dall·e': 1,\n",
       "  'highresolution': 1,\n",
       "  'renderings': 1,\n",
       "  'imaginary': 1,\n",
       "  'scenes': 1,\n",
       "  'or': 11,\n",
       "  'objects': 1,\n",
       "  'prompts': 1,\n",
       "  'due': 2,\n",
       "  'potential': 6,\n",
       "  'transform': 4,\n",
       "  'economists': 1,\n",
       "  'expect': 1,\n",
       "  'affect': 1,\n",
       "  'every': 1,\n",
       "  'part': 2,\n",
       "  'economy': 1,\n",
       "  'could': 4,\n",
       "  'lead': 3,\n",
       "  'increases': 1,\n",
       "  'economic': 2,\n",
       "  'growth': 2,\n",
       "  'similar': 2,\n",
       "  'industrial': 1,\n",
       "  'revolution': 1,\n",
       "  'my': 7,\n",
       "  'own': 1,\n",
       "  'i’ve': 2,\n",
       "  'looking': 1,\n",
       "  'assist': 2,\n",
       "  'researchers': 3,\n",
       "  'am': 2,\n",
       "  'currently': 2,\n",
       "  'working': 1,\n",
       "  'ought': 2,\n",
       "  'san': 1,\n",
       "  'francisco': 1,\n",
       "  'company': 1,\n",
       "  'developing': 1,\n",
       "  'openended': 1,\n",
       "  'elicit': 7,\n",
       "  'help': 1,\n",
       "  'answer': 1,\n",
       "  'minutes': 1,\n",
       "  'hours': 1,\n",
       "  'instead': 1,\n",
       "  'weeks': 1,\n",
       "  'months': 1,\n",
       "  'designed': 1,\n",
       "  'growing': 1,\n",
       "  'number': 3,\n",
       "  'specific': 2,\n",
       "  'relevant': 2,\n",
       "  'summarization': 2,\n",
       "  'labeling': 2,\n",
       "  'rephrasing': 1,\n",
       "  'brainstorming': 4,\n",
       "  'literature': 2,\n",
       "  'reviews': 1,\n",
       "  'found': 1,\n",
       "  'not': 10,\n",
       "  'surprisingly': 1,\n",
       "  'works': 1,\n",
       "  'some': 5,\n",
       "  'others': 2,\n",
       "  'rough': 1,\n",
       "  'around': 1,\n",
       "  'edges': 1,\n",
       "  'noisy': 2,\n",
       "  'results': 4,\n",
       "  'spotty': 1,\n",
       "  'accuracy': 1,\n",
       "  'promise': 1,\n",
       "  'future': 3,\n",
       "  'rephrase': 1,\n",
       "  'task': 1,\n",
       "  'useful': 3,\n",
       "  'lack': 1,\n",
       "  'integration': 1,\n",
       "  'apps': 1,\n",
       "  'renders': 1,\n",
       "  'impractical': 1,\n",
       "  'now': 6,\n",
       "  'great': 1,\n",
       "  'ideas': 1,\n",
       "  'identifying': 1,\n",
       "  'overlooked': 2,\n",
       "  'topics': 2,\n",
       "  'despite': 1,\n",
       "  'barriers': 1,\n",
       "  'adoption': 2,\n",
       "  'valuable': 2,\n",
       "  'variety': 3,\n",
       "  'situations': 1,\n",
       "  'all': 3,\n",
       "  'offers': 1,\n",
       "  'find': 2,\n",
       "  'review': 1,\n",
       "  'sort': 1,\n",
       "  'its': 4,\n",
       "  'breadandbutter': 1,\n",
       "  'when': 2,\n",
       "  'need': 4,\n",
       "  'start': 4,\n",
       "  'digging': 1,\n",
       "  'into': 3,\n",
       "  'new': 5,\n",
       "  'topic': 1,\n",
       "  'become': 1,\n",
       "  'goto': 1,\n",
       "  'resource': 1,\n",
       "  'spend': 1,\n",
       "  'less': 1,\n",
       "  'trying': 1,\n",
       "  'existing': 2,\n",
       "  'content': 1,\n",
       "  'applicable': 1,\n",
       "  'interfaces': 1,\n",
       "  'academic': 1,\n",
       "  'search': 1,\n",
       "  'google': 2,\n",
       "  'scholar': 1,\n",
       "  'beginning': 2,\n",
       "  'integrate': 1,\n",
       "  'experience': 2,\n",
       "  'inspired': 2,\n",
       "  'seeks': 1,\n",
       "  'utilize': 1,\n",
       "  'supporting': 1,\n",
       "  'strategic': 2,\n",
       "  'planning': 1,\n",
       "  'organizations': 5,\n",
       "  'prepare': 1,\n",
       "  'identify': 2,\n",
       "  'your': 20,\n",
       "  'assets': 6,\n",
       "  'determine': 1,\n",
       "  'techniques': 1,\n",
       "  'leveraged': 1,\n",
       "  'add': 2,\n",
       "  'value': 6,\n",
       "  'firm': 2,\n",
       "  'you': 14,\n",
       "  'certainly': 2,\n",
       "  'aware': 1,\n",
       "  'overlooking': 1,\n",
       "  'essential': 1,\n",
       "  'if': 2,\n",
       "  'utilizing': 1,\n",
       "  'throughout': 2,\n",
       "  'organization': 6,\n",
       "  'customer': 2,\n",
       "  'management': 1,\n",
       "  'understanding': 5,\n",
       "  'voice': 1,\n",
       "  'think': 2,\n",
       "  'about': 1,\n",
       "  'emails': 1,\n",
       "  'analysts’': 1,\n",
       "  'contracts': 1,\n",
       "  'press': 1,\n",
       "  'releases': 1,\n",
       "  'archives': 1,\n",
       "  'meetings': 1,\n",
       "  'phone': 1,\n",
       "  'calls': 1,\n",
       "  'transcribed': 1,\n",
       "  'there': 2,\n",
       "  'so': 1,\n",
       "  'don’t': 4,\n",
       "  'extract': 1,\n",
       "  'hugging': 1,\n",
       "  'face': 1,\n",
       "  'startup': 1,\n",
       "  'released': 1,\n",
       "  'autonlp': 1,\n",
       "  'automates': 1,\n",
       "  'training': 2,\n",
       "  'standard': 1,\n",
       "  'uploading': 1,\n",
       "  'platform': 1,\n",
       "  'needs': 2,\n",
       "  'labels': 1,\n",
       "  'far': 1,\n",
       "  'fewer': 2,\n",
       "  'applications': 2,\n",
       "  'many': 5,\n",
       "  'firms': 1,\n",
       "  'made': 1,\n",
       "  'ambitious': 1,\n",
       "  'bets': 1,\n",
       "  'only': 2,\n",
       "  'struggle': 1,\n",
       "  'drive': 1,\n",
       "  'core': 1,\n",
       "  'business': 5,\n",
       "  'remain': 1,\n",
       "  'cautious': 1,\n",
       "  'overzealous': 1,\n",
       "  'good': 1,\n",
       "  'step': 3,\n",
       "  'machine': 1,\n",
       "  'learning': 1,\n",
       "  'engineers': 1,\n",
       "  'talented': 1,\n",
       "  'scientists': 1,\n",
       "  'manage': 1,\n",
       "  'take': 1,\n",
       "  'again': 1,\n",
       "  'sectors': 2,\n",
       "  'divisions': 2,\n",
       "  'within': 1,\n",
       "  'use': 2,\n",
       "  'highly': 1,\n",
       "  'specialized': 3,\n",
       "  'vocabularies': 1,\n",
       "  'through': 1,\n",
       "  'combination': 1,\n",
       "  'open': 1,\n",
       "  'datasets': 1,\n",
       "  'train': 1,\n",
       "  'finance': 2,\n",
       "  'want': 2,\n",
       "  'customized': 1,\n",
       "  'commercial': 1,\n",
       "  'banking': 1,\n",
       "  'capital': 1,\n",
       "  'markets': 1,\n",
       "  'unlabeled': 1,\n",
       "  'unlock': 1,\n",
       "  'untold': 1,\n",
       "  'firmunderstand': 1,\n",
       "  'might': 3,\n",
       "  'leverage': 1,\n",
       "  'aibased': 1,\n",
       "  'technologies': 4,\n",
       "  'make': 2,\n",
       "  'decisions': 2,\n",
       "  'reorganize': 2,\n",
       "  'skilled': 2,\n",
       "  'labor': 2,\n",
       "  'won’t': 1,\n",
       "  'replace': 3,\n",
       "  'will': 9,\n",
       "  'automate': 1,\n",
       "  'makers': 1,\n",
       "  'startups': 1,\n",
       "  'verneek': 1,\n",
       "  'elicitlike': 1,\n",
       "  'enable': 1,\n",
       "  'everyone': 1,\n",
       "  'datainformed': 1,\n",
       "  'transcend': 1,\n",
       "  'intelligence': 5,\n",
       "  'roles': 3,\n",
       "  'just': 4,\n",
       "  'generation': 1,\n",
       "  'productive': 1,\n",
       "  'likely': 2,\n",
       "  'means': 1,\n",
       "  'dedicated': 1,\n",
       "  'employees': 2,\n",
       "  'modest': 1,\n",
       "  'using': 1,\n",
       "  'them': 1,\n",
       "  'increasing': 2,\n",
       "  'complex': 1,\n",
       "  'true': 1,\n",
       "  'software': 1,\n",
       "  'developers': 1,\n",
       "  'significant': 2,\n",
       "  'implications': 1,\n",
       "  'web': 1,\n",
       "  'development': 1,\n",
       "  'begin': 3,\n",
       "  'incorporating': 1,\n",
       "  'understand': 3,\n",
       "  'capabilities': 1,\n",
       "  'right': 2,\n",
       "  'surprising': 1,\n",
       "  'ways': 2,\n",
       "  'fact': 1,\n",
       "  'suggestion': 2,\n",
       "  'one': 1,\n",
       "  'elicit’s': 1,\n",
       "  'conditioned': 1,\n",
       "  'suggestions': 1,\n",
       "  'original': 1,\n",
       "  'itself': 1,\n",
       "  'wasn’t': 1,\n",
       "  'perfect': 1,\n",
       "  'reminded': 1,\n",
       "  'me': 1,\n",
       "  'had': 2,\n",
       "  'revised': 1,\n",
       "  'accordingly': 1,\n",
       "  'scenarioplanning': 1,\n",
       "  'exercises': 1,\n",
       "  'although': 1,\n",
       "  'tremendous': 3,\n",
       "  'relatively': 1,\n",
       "  'crude': 1,\n",
       "  'current': 2,\n",
       "  'state': 1,\n",
       "  'bottom': 1,\n",
       "  'line': 1,\n",
       "  'encourage': 1,\n",
       "  'broad': 1,\n",
       "  'difficult': 2,\n",
       "  'anticipate': 1,\n",
       "  'different': 2,\n",
       "  'levels': 1,\n",
       "  'way': 2,\n",
       "  'get': 1,\n",
       "  'leaders': 2,\n",
       "  'adopt': 3,\n",
       "  'yourselves': 1,\n",
       "  'bet': 1,\n",
       "  'boat': 1,\n",
       "  'out': 1,\n",
       "  'team': 1,\n",
       "  'gains': 1,\n",
       "  'then': 1,\n",
       "  'ahead': 1,\n",
       "  'competition': 1,\n",
       "  'remember': 1,\n",
       "  'poised': 3,\n",
       "  'managers': 3,\n",
       "  'who': 2,\n",
       "  'underestimate': 1,\n",
       "  'ailarge': 1,\n",
       "  'exhibit': 1,\n",
       "  'abilities': 1,\n",
       "  'generalize': 1,\n",
       "  'without': 1,\n",
       "  'taskspecific': 1,\n",
       "  'recent': 1,\n",
       "  'progress': 2,\n",
       "  'toward': 1,\n",
       "  'humanlevel': 1,\n",
       "  'generalization': 1,\n",
       "  'general': 4,\n",
       "  'artificial': 4,\n",
       "  'ultimate': 1,\n",
       "  'goals': 1,\n",
       "  'including': 1,\n",
       "  'those': 1,\n",
       "  'systems': 1,\n",
       "  'disruptive': 2,\n",
       "  'aidriven': 1,\n",
       "  'explosive': 1,\n",
       "  'would': 1,\n",
       "  'radically': 2,\n",
       "  'society': 3,\n",
       "  'skeptical': 1,\n",
       "  'prudent': 1,\n",
       "  'organizations’': 1,\n",
       "  'cognizant': 1,\n",
       "  'early': 1,\n",
       "  'signs': 1,\n",
       "  'consider': 1,\n",
       "  'former': 1,\n",
       "  'chief': 1,\n",
       "  'eric': 1,\n",
       "  'schmidt': 1,\n",
       "  'expects': 1,\n",
       "  '10–20': 1,\n",
       "  'uk': 1,\n",
       "  'took': 1,\n",
       "  'official': 1,\n",
       "  'position': 1,\n",
       "  'risks': 3,\n",
       "  'paid': 1,\n",
       "  'anthony': 1,\n",
       "  'fauci’s': 1,\n",
       "  '2017': 1,\n",
       "  'warning': 1,\n",
       "  'importance': 1,\n",
       "  'pandemic': 2,\n",
       "  'preparedness': 1,\n",
       "  'severe': 1,\n",
       "  'effects': 2,\n",
       "  'ensuing': 1,\n",
       "  'supply': 3,\n",
       "  'chain': 3,\n",
       "  'crisis': 3,\n",
       "  'avoided': 1,\n",
       "  'ignoring': 1,\n",
       "  'carries': 1,\n",
       "  'firms’': 1,\n",
       "  'inaction': 1,\n",
       "  'irresponsible': 1,\n",
       "  'widespread': 1,\n",
       "  'damaging': 1,\n",
       "  'eg': 1,\n",
       "  'inequality': 1,\n",
       "  'domainspecific': 1,\n",
       "  'automation': 1,\n",
       "  'however': 1,\n",
       "  'unlike': 1,\n",
       "  'societal': 1,\n",
       "  'changes': 1,\n",
       "  'irreversible': 1,\n",
       "  'accelerate': 1,\n",
       "  'should': 1,\n",
       "  'preparing': 2,\n",
       "  'capitalize': 1,\n",
       "  'avoid': 1,\n",
       "  'undesirable': 1,\n",
       "  'futures': 1,\n",
       "  'ensure': 1,\n",
       "  'equitably': 1,\n",
       "  'benefit': 1,\n",
       "  'here': 2,\n",
       "  'stay': 1,\n",
       "  'powerful': 1,\n",
       "  'generalizable': 1,\n",
       "  'tip': 1,\n",
       "  'iceberg': 1,\n",
       "  'multimodal': 1,\n",
       "  'modelbased': 1,\n",
       "  'involved': 1,\n",
       "  'aggressively': 1,\n",
       "  'quicker': 1,\n",
       "  'adjust': 1,\n",
       "  'move': 1,\n",
       "  'forget': 1,\n",
       "  'yourself': 1})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A test document. This document can be found at https://hbr.org/2022/04/the-power-of-natural-language-processing\n",
    "\n",
    "sents = pd.read_csv(\"sents.csv\")\n",
    "get_dtm(sents.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78\n",
      "681\n",
      "[('ago', 1), ('aiauthored', 1), ('an', 1), ('and', 1), ('article', 1), ('been', 1), ('blog', 1), ('feats', 1), ('few', 1), ('for', 1), ('gone', 1), ('guardian', 1), ('has', 1), ('have', 1), ('it', 1), ('possible', 1), ('posts', 1), ('that', 1), ('the', 1), ('to', 1), ('used', 1), ('viral', 1), ('weren’t', 1), ('write', 1), ('years', 1)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "text    It has been used to write an article for The G...\n",
       "Name: 3, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ago', 1.0), ('aiauthored', 1.0), ('an', 1.0), ('and', 1.0), ('article', 1.0), ('been', 1.0), ('blog', 1.0), ('feats', 1.0), ('few', 1.0), ('for', 1.0), ('gone', 1.0), ('guardian', 1.0), ('has', 1.0), ('have', 1.0), ('it', 1.0), ('possible', 1.0), ('posts', 1.0), ('that', 1.0), ('the', 1.0), ('to', 1.0), ('used', 1.0), ('viral', 1.0), ('weren’t', 1.0), ('write', 1.0), ('years', 1.0)]\n"
     ]
    }
   ],
   "source": [
    "dtm, all_words = get_dtm(sents.text)\n",
    "\n",
    "# Check if the array is correct\n",
    "print(len(dtm))\n",
    "print(len(all_words))\n",
    "# randomly check one sentence\n",
    "idx = 3\n",
    "\n",
    "# get the dictionary using the function in Q1\n",
    "vocab = tokenize(sents[\"text\"].loc[idx])\n",
    "print(sorted(vocab.items(), key = lambda item: item[0]))\n",
    "\n",
    "# get all non-zero entries in dtm[idx] and create a dictionary\n",
    "# these two dictionaries should be the same\n",
    "sents.loc[idx]\n",
    "all_words = list(all_words.keys())\n",
    "vocab1 ={all_words[j]: dtm[idx][j] for j in np.where(dtm[idx]>0)[0]}\n",
    "print(sorted(vocab1.items(), key = lambda item: item[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3 Analyze DTM Array \n",
    "\n",
    "\n",
    "**Don't use any loop in this task**. You should use array operations to take the advantage of high performance computing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function named `analyze_dtm(dtm, words)` which:\n",
    "* takes an array $dtm$ and $words$ as an input, where $dtm$ is the array you get in Q2 with a shape $(m \\times n)$, and $words$ contains an array of words corresponding to the columns of $dtm$.\n",
    "* calculates the sentence frequency for each word, say $j$, e.g. how many sentences contain word $j$. Save the result to array $df$ ($df$ has shape of $(n,)$ or $(1, n)$).\n",
    "* normalizes the word count per sentence: divides word count, i.e., $dtm_{i,j}$, by the total number of words in sentence $i$. Save the result as an array named $tf$ ($tf$ has shape of $(m,n)$).\n",
    "* for each $dtm_{i,j}$, calculates $tf\\_idf_{i,j} = \\frac{tf_{i, j}}{df_j}$, i.e., divide each normalized word count by the sentence frequency of the word. The reason is, if a word appears in most sentences, it does not have the discriminative power and often is called a `stop` word. The inverse of $df$ can downgrade the weight of such words. $tf\\_idf$ has shape of $(m,n)$\n",
    "* prints out the following:\n",
    "    \n",
    "    - the total number of words in the document represented by $dtm$\n",
    "    - the most frequent top 10 words in this document    \n",
    "    - words with the top 10 largest $df$ values (show words and their $df$ values)\n",
    "    - the longest sentence (i.e., the one with the most words)\n",
    "    - top-10 words with the largest $tf\\_idf$ values in the longest sentence (show words and values) \n",
    "* returns the $tf\\_idf$ array.\n",
    "\n",
    "\n",
    "\n",
    "Note, for all the steps, **do not use any loop**. Just use array functions and broadcasting for high performance computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dtm(dtm, words, sents):\n",
    "    tfidf = None\n",
    "    sentence= np.count_nonzero(dtm, axis=1)\n",
    "    df= np.count_nonzero(dtm, axis=0)\n",
    "    \n",
    "\n",
    "    tf=np.copy(dtm)\n",
    "    for i in range(70):\n",
    "        \n",
    "        tf[i]= dtm[i]/sentence[i]\n",
    "\n",
    "    tfidf=tf/df\n",
    "   \n",
    "    \n",
    "        \n",
    "    n,all_words = get_dtm(sents)\n",
    "    all_words=dict(all_words)\n",
    "   \n",
    " \n",
    "\n",
    "    total=np.sum(dtm)\n",
    "    print(f\"The total number of words:\",total)\n",
    "   \n",
    "    print()\n",
    "    #most_occur= all_words.most_common(10)\n",
    "    #tf=np.array[dtm]\n",
    "    #tf.argmax(axis=1)\n",
    "    #np.argsort(tf,axis=none)[:,-1]\n",
    "    import heapq\n",
    "    print(\"The top 10 frequent words:\")\n",
    "    \n",
    "    print(heapq.nlargest(10, all_words.items(), key=lambda x:x[1]) )\n",
    "    print()\n",
    "    \n",
    "\n",
    "    print(\"The longest sentence :\")\n",
    "    \n",
    "    print(max(open(\"sents.csv\"),key=len))\n",
    "    \n",
    "    from operator import itemgetter\n",
    "    test_dict= dict(all_words)\n",
    "    Largest_df = dict(sorted(test_dict.items(), key = itemgetter(1), reverse = True)[:10])\n",
    "    print(f\"The top 10 words with highest df values:\")\n",
    "    print(\"The top 10 value are  \" + str(Largest_df))\n",
    "    print()\n",
    "    a=max(open(\"sents.csv\"),key=len)\n",
    "    \n",
    "    \n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    \n",
    "    tfidf2 = TfidfVectorizer(tokenizer=tokenize, stop_words='english')\n",
    "    tfs = tfidf2.fit_transform(a.split(\",\"))\n",
    "    s = '\"Language models are already reshaping traditional text analytics, but GPT-3 was an especially pivotal language model because, at 10x larger than any previous model upon release, it was the first large language model, which enabled it to perform even more advanced tasks like programming and solving high school–level math problems.\"'\n",
    "    response = tfidf2.transform([s])\n",
    "    feature_names = tfidf2.get_feature_names()\n",
    "    feature_array = np.array(tfidf2.get_feature_names())\n",
    "    tfidf_sorting = np.argsort(response.toarray()).flatten()[::-1]\n",
    "\n",
    "    n = 10\n",
    "    top_n = feature_array[tfidf_sorting][:n]\n",
    "\n",
    "   \n",
    "\n",
    "   \n",
    "    \n",
    "\n",
    "    print(f\"The top words with highest tf-idf values in the longest sentece:\",top_n)\n",
    "    \n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of words: 1853.0\n",
      "\n",
      "The top 10 frequent words:\n",
      "[('the', 68), ('to', 65), ('and', 52), ('of', 50), ('for', 37), ('ai', 25), ('in', 24), ('is', 23), ('are', 22), ('tasks', 20)]\n",
      "\n",
      "The longest sentence :\n",
      "\"Language models are already reshaping traditional text analytics, but GPT-3 was an especially pivotal language model because, at 10x larger than any previous model upon release, it was the first large language model, which enabled it to perform even more advanced tasks like programming and solving high school–level math problems.\"\n",
      "\n",
      "The top 10 words with highest df values:\n",
      "The top 10 value are  {'the': 68, 'to': 65, 'and': 52, 'of': 50, 'for': 37, 'ai': 25, 'in': 24, 'is': 23, 'are': 22, 'tasks': 20}\n",
      "\n",
      "The top words with highest tf-idf values in the longest sentece: ['traditional' 'text' 'advanced' 'analytics' 'enabled' 'especially' 'gpt3'\n",
      " 'high' 'large' 'larger']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[3.62318841e-03, 1.66666667e-01, 4.62962963e-03, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [8.36120401e-04, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [1.01112235e-03, 0.00000000e+00, 1.29198966e-03, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       ...,\n",
       "       [2.17391304e-02, 0.00000000e+00, 2.77777778e-02, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [2.17391304e-02, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        1.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [2.17391304e-02, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 1.00000000e+00, 1.00000000e+00]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = np.array(all_words)\n",
    "\n",
    "analyze_dtm(dtm, words, sents.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4. Find keywords of the document (Bonus) \n",
    "\n",
    "Can you leverage $dtm$ array you generated to find a few keywords that can be used to tag this document? e.g., AI, language models, tools, etc.\n",
    "\n",
    "\n",
    "Please use a narrative to describe your ideas and also implement your ideas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of words: 1853.0\n",
      "\n",
      "The top 10 frequent words:\n",
      "[('the', 68), ('to', 65), ('and', 52), ('of', 50), ('for', 37), ('ai', 25), ('in', 24), ('is', 23), ('are', 22), ('tasks', 20)]\n",
      "\n",
      "The longest sentence :\n",
      "\"Language models are already reshaping traditional text analytics, but GPT-3 was an especially pivotal language model because, at 10x larger than any previous model upon release, it was the first large language model, which enabled it to perform even more advanced tasks like programming and solving high school–level math problems.\"\n",
      "\n",
      "The top 10 words with highest df values:\n",
      "The top 10 value are  {'the': 68, 'to': 65, 'and': 52, 'of': 50, 'for': 37, 'ai': 25, 'in': 24, 'is': 23, 'are': 22, 'tasks': 20}\n",
      "\n",
      "The top words with highest tf-idf values in the longest sentece: ['traditional' 'text' 'advanced' 'analytics' 'enabled' 'especially' 'gpt3'\n",
      " 'high' 'large' 'larger']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "words = np.array(all_words)\n",
    "tfidf= analyze_dtm(dtm, words, sents.text)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use highest tf-idf values to tag the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
